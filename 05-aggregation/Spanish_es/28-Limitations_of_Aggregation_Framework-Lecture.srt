1
00:00:00,000 --> 00:00:00,180

2
00:00:00,180 --> 00:00:02,580
OK. Hemos hablado ya por algún
tiempo sobre el Aggregation

3
00:00:02,580 --> 00:00:03,512
Framework y lo bueno que es.

4
00:00:03,512 --> 00:00:05,970
Pero ahora quiero hablarte sobre
algunas de las limitaciones

5
00:00:05,970 --> 00:00:09,514
que existen en el Aggregation
Framework dentro de MongoDB,

6
00:00:09,514 --> 00:00:11,180
cómo puedes hacer para
eludirlas y en qué

7
00:00:11,180 --> 00:00:13,210
debes de pensar cuando
estés usándolo.

8
00:00:13,210 --> 00:00:16,390
La primera limitación es
que por defecto, tenemos

9
00:00:16,390 --> 00:00:18,530
un límite de 100MB.

10
00:00:18,530 --> 00:00:22,570
Ése es un límite considerablemente
pequeño para las fases del "pipeline".

11
00:00:22,570 --> 00:00:24,910
Y la manera en que evitas esa
limitación es, por supuesto,

12
00:00:24,910 --> 00:00:29,449
usando la opción "allowDiskUse",
que te permitirá sobrepasar

13
00:00:29,449 --> 00:00:30,740
ese límite de 100MB.

14
00:00:30,740 --> 00:00:34,290
Pero a menos que especifiques
esa opción en tu agregación,

15
00:00:34,290 --> 00:00:36,440
vas a estar limitado a esos 100MB

16
00:00:36,440 --> 00:00:40,130
y vas a ver que tus
consultas no retornan

17
00:00:40,130 --> 00:00:43,750
si son muy grandes o si tienen
grandes resultados intermedios

18
00:00:43,750 --> 00:00:46,930
en las fases de agrupación
u ordenamiento.

19
00:00:46,930 --> 00:00:49,070
Ésa es la primera limitación.

20
00:00:49,070 --> 00:00:51,100
La segunda limitación
es que si decides

21
00:00:51,100 --> 00:00:54,050
retornar los resultados
en un solo documento,

22
00:00:54,050 --> 00:00:55,930
entonces puede haber
un límite de 16MB.

23
00:00:55,930 --> 00:00:59,900
Y como Python en efecto retorna
los resultados en un documento,

24
00:00:59,900 --> 00:01:03,410
hay un límite de 16MB
por defecto en Python.

25
00:01:03,410 --> 00:01:05,269
Vimos una manera fácil
de sortear esta limitación

26
00:01:05,269 --> 00:01:08,150
y consiste en, por supuesto,
simplemente pasar la opción

27
00:01:08,150 --> 00:01:11,020
"cursor = {}" al método "aggregate"

28
00:01:11,020 --> 00:01:14,440
y luego Python va a
retornar un cursor

29
00:01:14,440 --> 00:01:17,530
y puedes tener resultados
de agregación sin límite.

30
00:01:17,530 --> 00:01:18,915
Pero ten eso en mente.

31
00:01:18,915 --> 00:01:20,800
La última limitación de
la cual te quiero hablar

32
00:01:20,800 --> 00:01:22,400
tiene que ver con "sharding".

33
00:01:22,400 --> 00:01:28,880
En un sistema con Sharding, ni
bien usas un "$group" o un "$sort"

34
00:01:28,880 --> 00:01:32,710
o cualquier cosa que requiera
considerar todos los resultados,

35
00:01:32,710 --> 00:01:35,750
entonces lo que sucederá
es que los resultados

36
00:01:35,750 --> 00:01:37,470
serán devueltos
al primer "shard".

37
00:01:37,470 --> 00:01:39,850
Hablemos un poco
más sobre eso.

38
00:01:39,850 --> 00:01:43,630
Un sistema con Sharding
tiene múltiples "shards".

39
00:01:43,630 --> 00:01:47,490
Éste podría ser el "shard"
cero, "shard" uno, "shard" dos,

40
00:01:47,490 --> 00:01:51,230
"shard" tres, "shard" cuatro.

41
00:01:51,230 --> 00:01:53,230
En efecto, cada uno
de estos "shards"

42
00:01:53,230 --> 00:01:57,130
podría ser un conjunto de réplicas, así
que podría haber múltiples instancias

43
00:01:57,130 --> 00:01:58,910
de MongoDB detrás de ésta.

44
00:01:58,910 --> 00:02:01,460
Así que todo esto representa,
digamos, un conjunto de réplicas

45
00:02:01,460 --> 00:02:05,156
de tres nodos, el cual
es en sí un "shard".

46
00:02:05,156 --> 00:02:06,530
Cada uno de éstos
podría ser así.

47
00:02:06,530 --> 00:02:07,905
No los voy a dibujar,

48
00:02:07,905 --> 00:02:09,800
pero probablemente
son todos así.

49
00:02:09,800 --> 00:02:14,350
En esta situación, tienes
un router mongos en frente

50
00:02:14,350 --> 00:02:16,020
y tienes una aplicación.

51
00:02:16,020 --> 00:02:19,114
Esa aplicación va a enviar
una consulta de agregación

52
00:02:19,114 --> 00:02:21,530
y esa consulta, si utiliza
una colección con "sharding"

53
00:02:21,530 --> 00:02:24,440
va a enviarse a cada "shard".

54
00:02:24,440 --> 00:02:27,990
Algunas partes de la consulta,
digamos una proyección o un

55
00:02:27,990 --> 00:02:31,620
"$match", pueden ir en paralelo
en todos los "shards", pero si

56
00:02:31,620 --> 00:02:36,350
haces una fase "$group" o un "$sort"

57
00:02:36,350 --> 00:02:38,930
o cualquier cosa que requiera
ver todos los datos, entonces

58
00:02:38,930 --> 00:02:41,500
lo que va a ocurrir
es que MongoDB va a

59
00:02:41,500 --> 00:02:44,205
enviar todos los datos
al "shard" primario

60
00:02:44,205 --> 00:02:45,690
de la base de datos.

61
00:02:45,690 --> 00:02:48,460
El "shard" primario es donde
una colección sin "sharding"

62
00:02:48,460 --> 00:02:49,320
permanecería.

63
00:02:49,320 --> 00:02:51,670
Entonces, inicialmente
distribuirías el trabajo

64
00:02:51,670 --> 00:02:54,790
y si luego comienzas a hacer
un "$group" o un "$sort",

65
00:02:54,790 --> 00:02:56,290
entonces los resultados
van a ser enviados a

66
00:02:56,290 --> 00:02:59,369
un "shard" para un
procesamiento adicional

67
00:02:59,369 --> 00:03:01,410
que permite que todo sea
recolectado en un solo lugar.

68
00:03:01,410 --> 00:03:03,440
En ese sentido, puede
ser que no encuentres

69
00:03:03,440 --> 00:03:05,420
el mismo nivel de
escalabilidad en

70
00:03:05,420 --> 00:03:09,310
el Aggregation Framework que
digamos en Hadoop, donde

71
00:03:09,310 --> 00:03:11,620
puede haber mayor
paralelismo para

72
00:03:11,620 --> 00:03:13,395
consultas grandes, las que
llamamos trabajos de MapReduce.

73
00:03:13,395 --> 00:03:15,770
Para aquéllos que no están
familiarizados con MapReduce,

74
00:03:15,770 --> 00:03:17,830
eso no es importante ahora,

75
00:03:17,830 --> 00:03:20,630
pero la agregación es una interfaz
para funcionalidades de MapReduce

76
00:03:20,630 --> 00:03:23,850
dentro de MongoDB y puedes
ver que el rendimiento de

77
00:03:23,850 --> 00:03:26,135
una agregación grande en
un clúster con Sharding

78
00:03:26,135 --> 00:03:27,510
no sea tan bueno
como lo que esperarías

79
00:03:27,510 --> 00:03:30,042
si estás acostumbrado...

80
00:03:30,042 --> 00:03:31,500
digamos que estás ejecutando
una consulta grande en Hadoop.

81
00:03:31,500 --> 00:03:35,520
Lo que nos lleva a mencionar buenas
alternativas al Aggregation Framework.

82
00:03:35,520 --> 00:03:38,570
Éstas son que
puedes usar Hadoop,

83
00:03:38,570 --> 00:03:40,061
el cual ofrece MapReduce.

84
00:03:40,061 --> 00:03:41,685
Por supuesto que tienes que
exportar tus datos fuera

85
00:03:41,685 --> 00:03:44,300
de MongoDB e importarlos
a Hadoop, lo cual podrías

86
00:03:44,300 --> 00:03:46,750
hacer utilizando el
conector de Hadoop.

87
00:03:46,750 --> 00:03:50,150
Podrías utilizar el conector
de Hadoop para hacer eso.

88
00:03:50,150 --> 00:03:53,040
Y también hay otra funcionalidad
MapReduce que está incluida en

89
00:03:53,040 --> 00:03:55,680
MongoDB, la cual tiene
varias limitaciones,

90
00:03:55,680 --> 00:03:57,160
es un poco más vieja.

91
00:03:57,160 --> 00:03:59,940
En verdad, no recomendaría que uses esto.

92
00:03:59,940 --> 00:04:03,500
No lo recomiendo excepto
para cosas muy simples.

93
00:04:03,500 --> 00:04:06,090
Generalmente no recomiendo
la funcionalidad MapReduce

94
00:04:06,090 --> 00:04:07,320
incluida en MongoDB.

95
00:04:07,320 --> 00:04:08,720
El Aggregation Framework
es una mejor opción.

96
00:04:08,720 --> 00:04:12,300
Así que usa el Aggregation Framework,
no la funcionalidad MapReduce,

97
00:04:12,300 --> 00:04:15,070
pero también podrías utilizar
Hadoop como agregación alternativa,

98
00:04:15,070 --> 00:04:18,380
pero tienes que exportar los
datos de MongoDB y esencialmente

99
00:04:18,380 --> 00:04:22,790
importarlo a HDFS o usar nuestro
conector para conectar ambos.

100
00:04:22,790 --> 00:04:24,390
Muy bien. Esto fue
un resumen de las

101
00:04:24,390 --> 00:04:27,560
limitaciones del
Aggregation Framework.