1
00:00:00,000 --> 00:00:00,180

2
00:00:00,180 --> 00:00:02,580
OK, we've talked about
aggregation for a while now

3
00:00:02,580 --> 00:00:03,512
and how good it is.

4
00:00:03,512 --> 00:00:05,970
But now I want to talk to you
about some of the limitations

5
00:00:05,970 --> 00:00:09,514
that exist in the aggregation
framework within MongoDB

6
00:00:09,514 --> 00:00:11,180
and how you can get
around them and what

7
00:00:11,180 --> 00:00:13,210
you should think about
when you're using it.

8
00:00:13,210 --> 00:00:16,390
The first limitation
is that by default, we

9
00:00:16,390 --> 00:00:18,530
have a 100 megabyte limit.

10
00:00:18,530 --> 00:00:22,570
That's pretty small
for pipeline stages.

11
00:00:22,570 --> 00:00:24,910
And the way you get
around that, of course,

12
00:00:24,910 --> 00:00:29,449
is by using allow disk use
option, which will get you

13
00:00:29,449 --> 00:00:30,740
around that 100 megabyte limit.

14
00:00:30,740 --> 00:00:34,290
But unless you specify
that option to aggregation,

15
00:00:34,290 --> 00:00:36,440
you will be limited
to that 100 megabytes,

16
00:00:36,440 --> 00:00:40,130
and you will find that your
queries will not come back

17
00:00:40,130 --> 00:00:43,750
if they're very large, and they
have large intermediary results

18
00:00:43,750 --> 00:00:46,930
in the grouping or
the sorting stages.

19
00:00:46,930 --> 00:00:49,070
That's the first limitation.

20
00:00:49,070 --> 00:00:51,100
The second limitation
is that if you

21
00:00:51,100 --> 00:00:54,050
decide to return the
results in one document,

22
00:00:54,050 --> 00:00:55,930
then there can be a
16 megabyte limit.

23
00:00:55,930 --> 00:00:59,900
And since Python does return
the results in one document,

24
00:00:59,900 --> 00:01:03,410
there's a 16 megabyte
limit by default in Python.

25
00:01:03,410 --> 00:01:05,269
And we saw an easy
way around this one,

26
00:01:05,269 --> 00:01:08,150
and that's of
course to simply set

27
00:01:08,150 --> 00:01:11,020
cursor equal to
this empty document,

28
00:01:11,020 --> 00:01:14,440
and then Python
will return a cursor

29
00:01:14,440 --> 00:01:17,530
and you can have aggregation
results that have no limit.

30
00:01:17,530 --> 00:01:18,915
But keep that in mind.

31
00:01:18,915 --> 00:01:20,800
The last limitation I
want to tell you about

32
00:01:20,800 --> 00:01:22,400
has to do which sharding.

33
00:01:22,400 --> 00:01:28,880
So in a sharded system, as soon
as you use a group by or a sort

34
00:01:28,880 --> 00:01:32,710
or anything that requires
looking at all the results,

35
00:01:32,710 --> 00:01:35,750
then what will happen
is that the results

36
00:01:35,750 --> 00:01:37,470
will be brought back
to the first shard.

37
00:01:37,470 --> 00:01:39,850
So we'll talk a little
bit more about that.

38
00:01:39,850 --> 00:01:43,630
A sharded system
has multiple shards.

39
00:01:43,630 --> 00:01:47,490
And this might be
shard zero, shard one,

40
00:01:47,490 --> 00:01:51,230
shard two, shard
three, shard four.

41
00:01:51,230 --> 00:01:53,230
And in fact, each
of these shards

42
00:01:53,230 --> 00:01:57,130
may be a replica set, so
there may be multiple MongoDB

43
00:01:57,130 --> 00:01:58,910
standing behind this one.

44
00:01:58,910 --> 00:02:01,460
So all this represents
let's say a three node

45
00:02:01,460 --> 00:02:05,156
replica set that
is a single shard.

46
00:02:05,156 --> 00:02:06,530
Each of these
might be like that.

47
00:02:06,530 --> 00:02:07,905
So I'm not going
to draw them in,

48
00:02:07,905 --> 00:02:09,800
but they're probably
all like that.

49
00:02:09,800 --> 00:02:14,350
So in that situation, you've
got a Mongo s router in front,

50
00:02:14,350 --> 00:02:16,020
and you got an application.

51
00:02:16,020 --> 00:02:19,114
And that application is going
to send an aggregation query,

52
00:02:19,114 --> 00:02:21,530
and that aggregation query,
if it uses a collection that's

53
00:02:21,530 --> 00:02:24,440
sharded, is going to send
that to every single one.

54
00:02:24,440 --> 00:02:27,990
And some parts of that,
for instance a projection

55
00:02:27,990 --> 00:02:31,620
or a match, could go in
parallel with all the shards.

56
00:02:31,620 --> 00:02:36,350
But if you do a group by
phase for instance or a sort

57
00:02:36,350 --> 00:02:38,930
or anything else that requires
looking at all the data,

58
00:02:38,930 --> 00:02:41,500
then what's going
to happen is MongoDB

59
00:02:41,500 --> 00:02:44,205
is going to send all that
data to the primary shard

60
00:02:44,205 --> 00:02:45,690
for the database.

61
00:02:45,690 --> 00:02:48,460
And the primary shard is
where an unsharded collection

62
00:02:48,460 --> 00:02:49,320
would live.

63
00:02:49,320 --> 00:02:51,670
So initially you'll be
distributing the work,

64
00:02:51,670 --> 00:02:54,790
and then if you start
doing a group by or a sort,

65
00:02:54,790 --> 00:02:56,290
then the results
are going to get

66
00:02:56,290 --> 00:02:59,369
sent to one shard to
get processed further,

67
00:02:59,369 --> 00:03:01,410
so that everything can be
collected in one place.

68
00:03:01,410 --> 00:03:03,440
And so in that
sense, you may not

69
00:03:03,440 --> 00:03:05,420
find the same level
of scalability

70
00:03:05,420 --> 00:03:09,310
in aggregation that you might
find let's say Haddoup, where

71
00:03:09,310 --> 00:03:11,620
there might be greater
parallelism for very

72
00:03:11,620 --> 00:03:13,395
large what's called
MapReduce jobs.

73
00:03:13,395 --> 00:03:15,770
And for those of you who are
not familiar with MapReduce,

74
00:03:15,770 --> 00:03:17,830
it's not really
important right now,

75
00:03:17,830 --> 00:03:20,630
but aggregation is an interface
to MapReduce functionality

76
00:03:20,630 --> 00:03:23,850
within MongoDB, and you may
find that the performance

77
00:03:23,850 --> 00:03:26,135
of a large aggregation
on a sharded cluster

78
00:03:26,135 --> 00:03:27,510
may not be quite
as good as you'd

79
00:03:27,510 --> 00:03:30,042
expect if you are
used to, let's say,

80
00:03:30,042 --> 00:03:31,500
you're running a
large Haddoup job.

81
00:03:31,500 --> 00:03:35,520
Which gets to good
alternatives to aggregation

82
00:03:35,520 --> 00:03:38,570
and they are that you
could use Haddoup,

83
00:03:38,570 --> 00:03:40,061
which offers MapReduce.

84
00:03:40,061 --> 00:03:41,685
Of course you have
to get your data out

85
00:03:41,685 --> 00:03:44,300
of MongoDB and
into Haddoup, which

86
00:03:44,300 --> 00:03:46,750
you could do using
the Haddoup connector.

87
00:03:46,750 --> 00:03:50,150
So the Hadoup connector
could be used to do that.

88
00:03:50,150 --> 00:03:53,040
And there's also another
MapReduce functionality

89
00:03:53,040 --> 00:03:55,680
built into MongoDB, which
has several limitations,

90
00:03:55,680 --> 00:03:57,160
a little bit older.

91
00:03:57,160 --> 00:03:59,940
And I actually would not
recommend using this.

92
00:03:59,940 --> 00:04:03,500
So I don't recommend this
except for very simple stuff.

93
00:04:03,500 --> 00:04:06,090
I generally don't recommend
the MapReduce functionality

94
00:04:06,090 --> 00:04:07,320
built into MongoDB.

95
00:04:07,320 --> 00:04:08,720
Aggregation is a better choice.

96
00:04:08,720 --> 00:04:12,300
So use aggregation not
MapReduce functionality.

97
00:04:12,300 --> 00:04:15,070
But you could also use Haddoup
as an alternative aggregation,

98
00:04:15,070 --> 00:04:18,380
but you have to get data out
of MongoDB and essentially

99
00:04:18,380 --> 00:04:22,790
into HDFS or use our
connector to connect the two.

100
00:04:22,790 --> 00:04:24,390
All right, those
are the limitations

101
00:04:24,390 --> 00:04:27,560
of the aggregation
framework in a nutshell.