1
00:00:00,000 --> 00:00:00,400

2
00:00:00,400 --> 00:00:03,170
In this next section, we're
going to read in some data

3
00:00:03,170 --> 00:00:05,910
from Reddit just so we
can do some queries.

4
00:00:05,910 --> 00:00:08,260
And the reason I'm doing this is
really just to show you how

5
00:00:08,260 --> 00:00:11,360
easy it is to bring
data into MongoDB.

6
00:00:11,360 --> 00:00:14,570
So this is Reddit's technology
page right here.

7
00:00:14,570 --> 00:00:18,880
And it turns out that Reddit
very flexibly, if you add a

8
00:00:18,880 --> 00:00:24,260
.JSON to the end of any URL,
it will actually produce a

9
00:00:24,260 --> 00:00:28,030
JSON as the output of
any given page.

10
00:00:28,030 --> 00:00:28,870
What do you do with that?

11
00:00:28,870 --> 00:00:30,190
How would you bring
that into MongoDB?

12
00:00:30,190 --> 00:00:31,920
Well, it's pretty
straightforward.

13
00:00:31,920 --> 00:00:34,440
You just need to read that
URL and figure out

14
00:00:34,440 --> 00:00:35,060
what it looks like.

15
00:00:35,060 --> 00:00:36,490
So let's go into a terminal.

16
00:00:36,490 --> 00:00:39,850
And what I'm going to do is,
I'm going to curl that URL,

17
00:00:39,850 --> 00:00:43,150
reddit.com/r/technology, just
read it and send it to

18
00:00:43,150 --> 00:00:44,840
reddit.JSON.

19
00:00:44,840 --> 00:00:45,650
It did that.

20
00:00:45,650 --> 00:00:48,760
And if I go into reddit.JSON
and look at the file--

21
00:00:48,760 --> 00:00:51,530
I looked at this a little bit
before I recorded this.

22
00:00:51,530 --> 00:00:54,310
So I looked at the structure of
it and it looked like one

23
00:00:54,310 --> 00:00:55,890
big JSON document.

24
00:00:55,890 --> 00:00:59,030
And it looked like the most
interesting part here was that

25
00:00:59,030 --> 00:01:02,090
there was a children key here.

26
00:01:02,090 --> 00:01:04,879
In data there was a children
sub-document.

27
00:01:04,879 --> 00:01:08,520
And in there is a array of
every single story on the

28
00:01:08,520 --> 00:01:09,660
front page of Reddit.

29
00:01:09,660 --> 00:01:11,250
That's it.

30
00:01:11,250 --> 00:01:13,720
Now that I know that, it's
pretty straightforward for me

31
00:01:13,720 --> 00:01:15,090
to read it in.

32
00:01:15,090 --> 00:01:18,560
I just created a small script
here which uses a few

33
00:01:18,560 --> 00:01:22,250
different Python utilities that
you don't really need to

34
00:01:22,250 --> 00:01:24,210
know for this class, but I just
wanted to show you how

35
00:01:24,210 --> 00:01:25,780
they worked real quickly.

36
00:01:25,780 --> 00:01:28,040
So I'm importing PyMongo.

37
00:01:28,040 --> 00:01:30,140
I'm connecting to my
Mongo database.

38
00:01:30,140 --> 00:01:32,350
And I'm creating a new
connection to a database

39
00:01:32,350 --> 00:01:34,200
called Reddit, which doesn't
yet exist, so it's going to

40
00:01:34,200 --> 00:01:36,910
create it, and a collection
called Stories.

41
00:01:36,910 --> 00:01:41,360
Then I read the Reddit homepage
using urllib2, which

42
00:01:41,360 --> 00:01:44,750
is something you can install in
Python if it's installed.

43
00:01:44,750 --> 00:01:48,300
And then if you do that, you
can read any URL-- and I'm

44
00:01:48,300 --> 00:01:50,460
just reading the Reddit.com
URL, technology

45
00:01:50,460 --> 00:01:53,410
URL with .JSON appended.

46
00:01:53,410 --> 00:01:55,910
That gives you something that
looks like a file descriptor.

47
00:01:55,910 --> 00:01:57,480
And then I need to
interpret that.

48
00:01:57,480 --> 00:02:00,820
So what I do is I call a
Reddit.page Read, which gives

49
00:02:00,820 --> 00:02:02,780
me the entire contents
of the page.

50
00:02:02,780 --> 00:02:06,900
And then this JSON utility
will bring it into data

51
00:02:06,900 --> 00:02:09,940
structures inside Python,
so that are a bunch of

52
00:02:09,940 --> 00:02:12,600
dictionaries and
sub-dictionaries and whatnot.

53
00:02:12,600 --> 00:02:16,340
So now I've got what I'll call
the parsed version of that.

54
00:02:16,340 --> 00:02:21,580
Now all I do is, I look at every
item in this parsed--

55
00:02:21,580 --> 00:02:26,690
I know it is going to be an
array if I look at the

56
00:02:26,690 --> 00:02:30,730
sub-document data and its
sub-document children.

57
00:02:30,730 --> 00:02:32,900
I looked through the
output before.

58
00:02:32,900 --> 00:02:33,840
I know that's an array.

59
00:02:33,840 --> 00:02:36,190
So I know that's going to be
something I can iterate over.

60
00:02:36,190 --> 00:02:38,975
And every one of those, I looked
at the structure, and

61
00:02:38,975 --> 00:02:41,750
it looked like, if I took each
of those items and inserted

62
00:02:41,750 --> 00:02:43,850
item data dictionary element,
that it would

63
00:02:43,850 --> 00:02:45,040
be the story itself.

64
00:02:45,040 --> 00:02:47,920
So I cleaned it up just a little
bit by looking at data

65
00:02:47,920 --> 00:02:49,130
beforehand.

66
00:02:49,130 --> 00:02:50,460
So we do that.

67
00:02:50,460 --> 00:02:52,510
And then I'm going to run it.

68
00:02:52,510 --> 00:02:53,820
So let's see.

69
00:02:53,820 --> 00:02:57,930
That script was called
read_reddit.py.

70
00:02:57,930 --> 00:03:02,180
So I'm going to run that,
read_reddit.py.

71
00:03:02,180 --> 00:03:02,900
Very nice.

72
00:03:02,900 --> 00:03:04,390
It seems to have worked.

73
00:03:04,390 --> 00:03:08,070
And then I'm going to connect to
Mongo, and I'm going to use

74
00:03:08,070 --> 00:03:09,160
the Reddit database.

75
00:03:09,160 --> 00:03:11,230
And I'm going to look in
db.stories.find().pretty.

76
00:03:11,230 --> 00:03:14,610

77
00:03:14,610 --> 00:03:15,810
Let's see what I see.

78
00:03:15,810 --> 00:03:20,120
And now you can see what we
have here is every single

79
00:03:20,120 --> 00:03:23,120
story from the homepage
of Reddit.

80
00:03:23,120 --> 00:03:24,240
So that's nice.

81
00:03:24,240 --> 00:03:27,200
And it has things like title,
install Google search.

82
00:03:27,200 --> 00:03:30,160
This is exactly the same content
that was on Reddit's

83
00:03:30,160 --> 00:03:33,110
home page on their technology
homepage.

84
00:03:33,110 --> 00:03:34,520
I'll show you that.

85
00:03:34,520 --> 00:03:36,970
These are the exact same
stories, but now they've been

86
00:03:36,970 --> 00:03:38,900
parsed into Mongo.

87
00:03:38,900 --> 00:03:41,620
And the nice thing about this
that we didn't even need to

88
00:03:41,620 --> 00:03:46,820
think about what fields existed
in the structure.

89
00:03:46,820 --> 00:03:48,110
Mongo is very flexible.

90
00:03:48,110 --> 00:03:52,390
And so I was able to bring it in
and now I can query on it.

91
00:03:52,390 --> 00:03:55,140
So in the next lessons, we're
going to use some more query

92
00:03:55,140 --> 00:03:57,750
operators, but we're going to
use this more interesting data

93
00:03:57,750 --> 00:04:01,880
set that we got from Reddit
using basically a couple

94
00:04:01,880 --> 00:04:05,690
libraries, urllib2 to grab the
URL, a JSON library to parse

95
00:04:05,690 --> 00:04:08,380
it, and then just using a little
bit of knowledge of

96
00:04:08,380 --> 00:04:11,170
what it looks like, I was able
to just insert each of those

97
00:04:11,170 --> 00:04:12,440
stories into Mongo.

98
00:04:12,440 --> 00:04:13,690